{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import spacy\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# f = open('../../data/sentiment/positive')\n",
    "# pos = f.read()\n",
    "# f.close()\n",
    "\n",
    "# f = open('../../data/sentiment/negative')\n",
    "# neg = f.read()\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Spacy word embeddings\n",
    "word_embeddings = spacy.load('en', vectors='glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 300)\n"
     ]
    }
   ],
   "source": [
    "# Create a function to get vector format data for a sequence\n",
    "def sequence_to_data(seq, max_len=None):\n",
    "    seq = unicode(seq)\n",
    "    data = [word_embeddings(ix).vector for ix in seq.split()]\n",
    "    \n",
    "    if max_len is None:\n",
    "        max_len = len(data)\n",
    "    \n",
    "    data_mat = np.zeros((1, max_len, 300))\n",
    "    \n",
    "    for ix in range(min(len(data), max_len)):\n",
    "        data_mat[:, ix, :] = data[ix]\n",
    "    \n",
    "    return data_mat\n",
    "\n",
    "def seq_data_matrix(seq_data, max_len=None):\n",
    "    n_seq = len(seq_data)\n",
    "    data = np.concatenate([sequence_to_data(ix, max_len) for ix in seq_data], axis=0)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "q = sequence_to_data(u'hello! what is the date today?', 100)\n",
    "print q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame([], columns=['text', 'score'])\n",
    "# for ix in pos.split('\\n'):\n",
    "#     text = ix.strip().lower()\n",
    "#     if len(text) > 1:\n",
    "#         df = df.append({'text': text, 'score': 1}, ignore_index=True)\n",
    "#     # print sequence_to_data(ix.strip().lower()).shape\n",
    "\n",
    "# for ix in neg.split('\\n'):\n",
    "#     text = ix.strip().lower()\n",
    "#     if len(text) > 1:\n",
    "#         df = df.append({'text': text, 'score': 0}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df = sklearn.utils.shuffle(df).reset_index(drop=True)\n",
    "df = pd.read_csv('../../data/sentiment/dataset.csv', sep='|', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a gushy episode of  m  a  s  h  only this time...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the storys pathetic and the gags are puerile</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not only a comingofage story and cautionary pa...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beyond a handful of mildly amusing lines    th...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a complex psychological drama about a father w...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score\n",
       "0  a gushy episode of  m  a  s  h  only this time...    0.0\n",
       "1       the storys pathetic and the gags are puerile    0.0\n",
       "2  not only a comingofage story and cautionary pa...    1.0\n",
       "3  beyond a handful of mildly amusing lines    th...    0.0\n",
       "4  a complex psychological drama about a father w...    1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame([], columns=['x'])\n",
    "\n",
    "for ix in range(100):\n",
    "    a = a.append({'x': ix}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>x_sq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x  x_sq\n",
       "0  0.0   0.0\n",
       "1  1.0   1.0\n",
       "2  2.0   4.0\n",
       "3  3.0   9.0\n",
       "4  4.0  16.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['x_sq'] = a.x.apply(lambda x: x**2)\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['len'] = df['text'].str.split().apply(lambda x: len(x))\n",
    "# df = df.sort_index(ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a gushy episode of  m  a  s  h  only this time...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the storys pathetic and the gags are puerile</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not only a comingofage story and cautionary pa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beyond a handful of mildly amusing lines    th...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a complex psychological drama about a father w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score  len\n",
       "0  a gushy episode of  m  a  s  h  only this time...    0.0   15\n",
       "1       the storys pathetic and the gags are puerile    0.0    8\n",
       "2  not only a comingofage story and cautionary pa...    1.0   15\n",
       "3  beyond a handful of mildly amusing lines    th...    0.0   14\n",
       "4  a complex psychological drama about a father w...    1.0   16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.to_csv('../../data/sentiment/dataset.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bucket_sizes = [[0, 10], [10, 15], [15, 20], [20, 25], [25, 45]]\n",
    "\n",
    "def assign_bucket(x):\n",
    "    for bucket in bucket_sizes:\n",
    "        if x > bucket[0] and x <= bucket[1]:\n",
    "            return bucket_sizes.index(bucket)\n",
    "    return len(bucket_sizes)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>len</th>\n",
       "      <th>bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a gushy episode of  m  a  s  h  only this time...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the storys pathetic and the gags are puerile</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not only a comingofage story and cautionary pa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>beyond a handful of mildly amusing lines    th...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a complex psychological drama about a father w...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score  len  bucket\n",
       "0  a gushy episode of  m  a  s  h  only this time...    0.0   15       1\n",
       "1       the storys pathetic and the gags are puerile    0.0    8       0\n",
       "2  not only a comingofage story and cautionary pa...    1.0   15       1\n",
       "3  beyond a handful of mildly amusing lines    th...    0.0   14       1\n",
       "4  a complex psychological drama about a father w...    1.0   16       2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bucket'] = df.len.apply(assign_bucket)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>len</th>\n",
       "      <th>bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9136</th>\n",
       "      <td>a brisk  reverent  and subtly different sequel</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>too silly to take seriously</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6700</th>\n",
       "      <td>the date movie that franz kafka would have made</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3929</th>\n",
       "      <td>an inexperienced director  mehta has much to l...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8836</th>\n",
       "      <td>an uneven mix of dark satire and childhood awa...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  score  len  bucket\n",
       "9136     a brisk  reverent  and subtly different sequel    1.0    7       0\n",
       "1777                        too silly to take seriously    0.0    5       0\n",
       "6700    the date movie that franz kafka would have made    1.0    9       0\n",
       "3929  an inexperienced director  mehta has much to l...    0.0    8       0\n",
       "8836  an uneven mix of dark satire and childhood awa...    0.0    9       0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sort(columns=['bucket'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>len</th>\n",
       "      <th>bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9136</th>\n",
       "      <td>a brisk  reverent  and subtly different sequel</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>too silly to take seriously</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6700</th>\n",
       "      <td>the date movie that franz kafka would have made</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3929</th>\n",
       "      <td>an inexperienced director  mehta has much to l...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8836</th>\n",
       "      <td>an uneven mix of dark satire and childhood awa...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>smothered by its own solemnity</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8832</th>\n",
       "      <td>an absorbing documentary</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3921</th>\n",
       "      <td>it took 19 predecessors to get this</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8828</th>\n",
       "      <td>reggios continual visual barrage is absorbing ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8825</th>\n",
       "      <td>a stylistic romp thats always fun to watch</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6706</th>\n",
       "      <td>a pleasant romantic comedy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8822</th>\n",
       "      <td>credibility levels are low and character devel...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>a big fat pain</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8816</th>\n",
       "      <td>message movie or an actionpacked submarine spe...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918</th>\n",
       "      <td>a terrifically entertaining specimen of spielb...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8814</th>\n",
       "      <td>the acting alone is worth the price of admission</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8845</th>\n",
       "      <td>parris performance is credible and remarkably ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3914</th>\n",
       "      <td>a refreshingly realistic  affectationfree comi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>just as moving  uplifting and funny as ever</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>a hypnotic portrait of this sad  compulsive life</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8867</th>\n",
       "      <td>the film is small in scope  yet perfectly formed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>its unlikely well see a better thriller this year</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6690</th>\n",
       "      <td>audaciousimpossible yet compelling</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6691</th>\n",
       "      <td>the fetid underbelly of fame has never looked ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8862</th>\n",
       "      <td>a perfectly respectable  perfectly inoffensive...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>a cheap  ludicrous attempt at serious horror</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8860</th>\n",
       "      <td>nasty  ugly  pointless and depressing  even if...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8859</th>\n",
       "      <td>whats next   my mother the car</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>terrible</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6694</th>\n",
       "      <td>the movie doesnt add anything fresh to the myth</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9623</th>\n",
       "      <td>one of the most significant moviegoing pleasur...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>its got the brawn  but not the brains</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>an ambitious  guiltsuffused melodrama crippled...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9647</th>\n",
       "      <td>involving at times  but lapses quite casually ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>neither funny nor suspenseful nor particularly...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9663</th>\n",
       "      <td>a subformulaic slap in the face to seasonal cheer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>its not original enough</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>thoughtful  provocative and entertaining</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4519</th>\n",
       "      <td>the best part about  gangs  was daniel daylewis</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4506</th>\n",
       "      <td>a nearly 212 hours  the film is way too indulgent</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4505</th>\n",
       "      <td>a soso  madefortv something posing as a real m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>the plot grinds on with yawnprovoking dullness</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4503</th>\n",
       "      <td>technically and artistically inept</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>parents beware  this is downright movie penance</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5219</th>\n",
       "      <td>barrels along at the start before becoming mir...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6092</th>\n",
       "      <td>samuel beckett applied to the iranian voting p...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6088</th>\n",
       "      <td>a witty  whimsical feature debut</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10317</th>\n",
       "      <td>interminably bleak  to say nothing of boring</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4502</th>\n",
       "      <td>an incredibly heavyhanded  manipulative dud th...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10541</th>\n",
       "      <td>a must see for all sides of the political spec...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10223</th>\n",
       "      <td>first good  then bothersome  excellent acting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068</th>\n",
       "      <td>a poorly executed comedy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9664</th>\n",
       "      <td>a very charming and funny movie</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4518</th>\n",
       "      <td>dark and disturbing  but also surprisingly funny</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>a delightful comingofage story</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5221</th>\n",
       "      <td>elvira fans could hardly ask for more</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5396</th>\n",
       "      <td>an inviting piece of film</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>a derivative collection of horror and scifi cl...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6096</th>\n",
       "      <td>a directtovoid release  heading nowhere</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4514</th>\n",
       "      <td>no way i can believe this load of junk</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2049 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  score  len  bucket\n",
       "9136      a brisk  reverent  and subtly different sequel    1.0    7       0\n",
       "1777                         too silly to take seriously    0.0    5       0\n",
       "6700     the date movie that franz kafka would have made    1.0    9       0\n",
       "3929   an inexperienced director  mehta has much to l...    0.0    8       0\n",
       "8836   an uneven mix of dark satire and childhood awa...    0.0    9       0\n",
       "1789                      smothered by its own solemnity    0.0    5       0\n",
       "8832                            an absorbing documentary    1.0    3       0\n",
       "3921                 it took 19 predecessors to get this    1.0    7       0\n",
       "8828   reggios continual visual barrage is absorbing ...    1.0   10       0\n",
       "8825          a stylistic romp thats always fun to watch    1.0    8       0\n",
       "6706                          a pleasant romantic comedy    1.0    4       0\n",
       "8822   credibility levels are low and character devel...    0.0    9       0\n",
       "1797                                      a big fat pain    0.0    4       0\n",
       "8816   message movie or an actionpacked submarine spe...    0.0   10       0\n",
       "3918   a terrifically entertaining specimen of spielb...    1.0    7       0\n",
       "8814    the acting alone is worth the price of admission    1.0    9       0\n",
       "8845   parris performance is credible and remarkably ...    1.0    7       0\n",
       "3914   a refreshingly realistic  affectationfree comi...    1.0    6       0\n",
       "1775         just as moving  uplifting and funny as ever    1.0    8       0\n",
       "1773    a hypnotic portrait of this sad  compulsive life    1.0    8       0\n",
       "8867    the film is small in scope  yet perfectly formed    1.0    9       0\n",
       "3946   its unlikely well see a better thriller this year    1.0    9       0\n",
       "6690                  audaciousimpossible yet compelling    1.0    3       0\n",
       "6691   the fetid underbelly of fame has never looked ...    0.0    9       0\n",
       "8862   a perfectly respectable  perfectly inoffensive...    1.0    8       0\n",
       "1759        a cheap  ludicrous attempt at serious horror    0.0    7       0\n",
       "8860   nasty  ugly  pointless and depressing  even if...    0.0   10       0\n",
       "8859                      whats next   my mother the car    0.0    6       0\n",
       "3942                                            terrible    0.0    1       0\n",
       "6694     the movie doesnt add anything fresh to the myth    0.0    9       0\n",
       "...                                                  ...    ...  ...     ...\n",
       "9623   one of the most significant moviegoing pleasur...    1.0   10       0\n",
       "5650               its got the brawn  but not the brains    0.0    8       0\n",
       "1010   an ambitious  guiltsuffused melodrama crippled...    0.0    8       0\n",
       "9647   involving at times  but lapses quite casually ...    0.0   10       0\n",
       "4497   neither funny nor suspenseful nor particularly...    0.0    7       0\n",
       "9663   a subformulaic slap in the face to seasonal cheer    0.0    9       0\n",
       "108                              its not original enough    0.0    4       0\n",
       "469             thoughtful  provocative and entertaining    1.0    4       0\n",
       "4519     the best part about  gangs  was daniel daylewis    1.0    8       0\n",
       "4506   a nearly 212 hours  the film is way too indulgent    0.0   10       0\n",
       "4505   a soso  madefortv something posing as a real m...    0.0    9       0\n",
       "1000      the plot grinds on with yawnprovoking dullness    0.0    7       0\n",
       "4503                  technically and artistically inept    0.0    4       0\n",
       "999      parents beware  this is downright movie penance    0.0    7       0\n",
       "5219   barrels along at the start before becoming mir...    0.0   10       0\n",
       "6092   samuel beckett applied to the iranian voting p...    1.0    8       0\n",
       "6088                    a witty  whimsical feature debut    1.0    5       0\n",
       "10317       interminably bleak  to say nothing of boring    0.0    7       0\n",
       "4502   an incredibly heavyhanded  manipulative dud th...    0.0   10       0\n",
       "10541  a must see for all sides of the political spec...    1.0   10       0\n",
       "10223  first good  then bothersome  excellent acting ...    0.0    8       0\n",
       "6068                            a poorly executed comedy    0.0    4       0\n",
       "9664                     a very charming and funny movie    1.0    6       0\n",
       "4518    dark and disturbing  but also surprisingly funny    1.0    7       0\n",
       "107                       a delightful comingofage story    1.0    4       0\n",
       "5221               elvira fans could hardly ask for more    1.0    7       0\n",
       "5396                           an inviting piece of film    1.0    5       0\n",
       "110    a derivative collection of horror and scifi cl...    0.0    8       0\n",
       "6096             a directtovoid release  heading nowhere    0.0    5       0\n",
       "4514              no way i can believe this load of junk    0.0    9       0\n",
       "\n",
       "[2049 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.bucket == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_batch(data, batch_size=10, gpu=True):\n",
    "    for bx in range(len(bucket_sizes)):\n",
    "        bucket_data = df[(df.bucket == bx)].reset_index(drop=True)\n",
    "        # print bx, bucket_sizes[bx][1], bucket_data.shape\n",
    "        \n",
    "        start = 0\n",
    "        stop = start + batch_size\n",
    "        \n",
    "        while start < bucket_data.shape[0]:\n",
    "            seq_length = bucket_sizes[bx][1]\n",
    "            section = bucket_data[start:stop]\n",
    "            X_data = seq_data_matrix(section.text, max_len=seq_length)\n",
    "            y_data = section.score\n",
    "            \n",
    "            if gpu:\n",
    "                yield Variable(torch.FloatTensor(X_data).cuda(), requires_grad=True), Variable(torch.LongTensor(y_data)).cuda()\n",
    "            else:\n",
    "                yield Variable(torch.FloatTensor(X_data), requires_grad=True), Variable(torch.LongTensor(y_data))\n",
    "            \n",
    "            start = stop\n",
    "            stop = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 10, 300]) torch.Size([1000])\n",
      "torch.Size([1000, 10, 300]) torch.Size([1000])\n",
      "torch.Size([49, 10, 300]) torch.Size([49])\n",
      "torch.Size([1000, 15, 300]) torch.Size([1000])\n",
      "torch.Size([1000, 15, 300]) torch.Size([1000])\n",
      "torch.Size([81, 15, 300]) torch.Size([81])\n",
      "torch.Size([1000, 20, 300]) torch.Size([1000])\n",
      "torch.Size([1000, 20, 300]) torch.Size([1000])\n",
      "torch.Size([333, 20, 300]) torch.Size([333])\n",
      "torch.Size([1000, 25, 300]) torch.Size([1000])\n",
      "torch.Size([968, 25, 300]) torch.Size([968])\n",
      "torch.Size([1000, 45, 300]) torch.Size([1000])\n",
      "torch.Size([1000, 45, 300]) torch.Size([1000])\n",
      "torch.Size([231, 45, 300]) torch.Size([231])\n"
     ]
    }
   ],
   "source": [
    "for ix, iy in make_batch(df, batch_size=1000):\n",
    "    print ix.shape, iy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.head(10)\n",
    "# Printing colored text (Useful later)\n",
    "# print colored(\"hello red world\", 'blue')# print 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SeqModel(nn.Module):\n",
    "    def __init__(self, in_shape=None, out_shape=None, hidden_shape=None):\n",
    "        super(SeqModel, self).__init__()\n",
    "        self.in_shape = in_shape\n",
    "        self.out_shape = out_shape\n",
    "        self.hidden_shape = hidden_shape\n",
    "        self.n_layers = 1\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.in_shape,\n",
    "            hidden_size=self.hidden_shape,\n",
    "            num_layers=self.n_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.lin = nn.Linear(self.hidden_shape, 64)\n",
    "        self.dropout = nn.Dropout(0.42)\n",
    "        self.out = nn.Linear(64, self.out_shape)\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        r_out, h_state = self.rnn(x, h)\n",
    "        last_out = r_out[:, -1, :]\n",
    "        y = F.tanh(self.lin(last_out))\n",
    "        y = self.dropout(y)\n",
    "        y = F.softmax(self.out(y))\n",
    "        return y\n",
    "    \n",
    "    def predict(self, x):\n",
    "        h_state = self.init_hidden(1, gpu=False)\n",
    "        \n",
    "        x = sequence_to_data(x)\n",
    "        pred = self.forward(torch.FloatTensor(x), h_state)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        h_state = self.init_hidden(1, gpu=False)\n",
    "        \n",
    "        x = sequence_to_data(x)\n",
    "        r_out, h = self.rnn(torch.FloatTensor(x), h_state)\n",
    "        last_out = r_out[:, -1, :]\n",
    "        \n",
    "        return last_out.data.numpy()\n",
    "            \n",
    "    def init_hidden(self, batch_size, gpu=True):\n",
    "        if gpu:\n",
    "            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_shape).cuda()),\n",
    "                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_shape)).cuda())\n",
    "        return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_shape)),\n",
    "                Variable(torch.zeros(self.n_layers, batch_size, self.hidden_shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeqModel(\n",
      "  (rnn): LSTM(300, 256, batch_first=True)\n",
      "  (lin): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (dropout): Dropout(p=0.42)\n",
      "  (out): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SeqModel(\n",
       "  (rnn): LSTM(300, 256, batch_first=True)\n",
       "  (lin): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (dropout): Dropout(p=0.42)\n",
       "  (out): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SeqModel(in_shape=300, hidden_shape=256, out_shape=2)\n",
    "\n",
    "print model\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.predict('hello bad world')\n",
    "\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load('/home/shubham/all_projects/CB/Summer_2018/data/checkpoints/seq_lstm_bucket/model_256h_epoch_700.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.69742333889 at Epoch: 0 | Step: 0\n",
      "Loss: 0.69163531065 at Epoch: 0 | Step: 20\n",
      "Loss: 0.692053377628 at Epoch: 0 | Step: 40\n",
      "Overall Average Loss: 0.694422006607 at Epoch: 0\n",
      "Loss: 0.692090451717 at Epoch: 1 | Step: 0\n",
      "Loss: 0.687447428703 at Epoch: 1 | Step: 20\n",
      "Loss: 0.689199209213 at Epoch: 1 | Step: 40\n",
      "Overall Average Loss: 0.689712703228 at Epoch: 1\n",
      "Loss: 0.681926727295 at Epoch: 2 | Step: 0\n",
      "Loss: 0.668727934361 at Epoch: 2 | Step: 20\n",
      "Loss: 0.647350132465 at Epoch: 2 | Step: 40\n",
      "Overall Average Loss: 0.674889683723 at Epoch: 2\n",
      "Loss: 0.63601154089 at Epoch: 3 | Step: 0\n",
      "Loss: 0.608234405518 at Epoch: 3 | Step: 20\n",
      "Loss: 0.593071639538 at Epoch: 3 | Step: 40\n",
      "Overall Average Loss: 0.638657689095 at Epoch: 3\n",
      "Loss: 0.577009916306 at Epoch: 4 | Step: 0\n",
      "Loss: 0.578892111778 at Epoch: 4 | Step: 20\n",
      "Loss: 0.581489622593 at Epoch: 4 | Step: 40\n",
      "Overall Average Loss: 0.612688839436 at Epoch: 4\n",
      "Loss: 0.549013614655 at Epoch: 5 | Step: 0\n",
      "Loss: 0.570038259029 at Epoch: 5 | Step: 20\n",
      "Loss: 0.575555443764 at Epoch: 5 | Step: 40\n",
      "Overall Average Loss: 0.593836128712 at Epoch: 5\n",
      "Loss: 0.526914358139 at Epoch: 6 | Step: 0\n",
      "Loss: 0.56396317482 at Epoch: 6 | Step: 20\n",
      "Loss: 0.570190310478 at Epoch: 6 | Step: 40\n",
      "Overall Average Loss: 0.584912419319 at Epoch: 6\n",
      "Loss: 0.51433557272 at Epoch: 7 | Step: 0\n",
      "Loss: 0.557445347309 at Epoch: 7 | Step: 20\n",
      "Loss: 0.571662664413 at Epoch: 7 | Step: 40\n",
      "Overall Average Loss: 0.582174420357 at Epoch: 7\n",
      "Loss: 0.525627613068 at Epoch: 8 | Step: 0\n",
      "Loss: 0.543232917786 at Epoch: 8 | Step: 20\n",
      "Loss: 0.556832849979 at Epoch: 8 | Step: 40\n",
      "Overall Average Loss: 0.57739084959 at Epoch: 8\n",
      "Loss: 0.525994777679 at Epoch: 9 | Step: 0\n",
      "Loss: 0.545738399029 at Epoch: 9 | Step: 20\n",
      "Loss: 0.544856071472 at Epoch: 9 | Step: 40\n",
      "Overall Average Loss: 0.570654571056 at Epoch: 9\n",
      "Loss: 0.513692259789 at Epoch: 10 | Step: 0\n",
      "Loss: 0.541399240494 at Epoch: 10 | Step: 20\n",
      "Loss: 0.542103528976 at Epoch: 10 | Step: 40\n",
      "Overall Average Loss: 0.563399195671 at Epoch: 10\n",
      "Loss: 0.505694925785 at Epoch: 11 | Step: 0\n",
      "Loss: 0.533120155334 at Epoch: 11 | Step: 20\n",
      "Loss: 0.536459207535 at Epoch: 11 | Step: 40\n",
      "Overall Average Loss: 0.557379901409 at Epoch: 11\n",
      "Loss: 0.493396878242 at Epoch: 12 | Step: 0\n",
      "Loss: 0.526925086975 at Epoch: 12 | Step: 20\n",
      "Loss: 0.531074225903 at Epoch: 12 | Step: 40\n",
      "Overall Average Loss: 0.551092982292 at Epoch: 12\n",
      "Loss: 0.489806681871 at Epoch: 13 | Step: 0\n",
      "Loss: 0.519273877144 at Epoch: 13 | Step: 20\n",
      "Loss: 0.519293963909 at Epoch: 13 | Step: 40\n",
      "Overall Average Loss: 0.545694589615 at Epoch: 13\n",
      "Loss: 0.48345375061 at Epoch: 14 | Step: 0\n",
      "Loss: 0.51608812809 at Epoch: 14 | Step: 20\n",
      "Loss: 0.513243317604 at Epoch: 14 | Step: 40\n",
      "Overall Average Loss: 0.541397333145 at Epoch: 14\n",
      "Loss: 0.481462657452 at Epoch: 15 | Step: 0\n",
      "Loss: 0.510341882706 at Epoch: 15 | Step: 20\n",
      "Loss: 0.511853575706 at Epoch: 15 | Step: 40\n",
      "Overall Average Loss: 0.537546873093 at Epoch: 15\n",
      "Loss: 0.480911433697 at Epoch: 16 | Step: 0\n",
      "Loss: 0.513807117939 at Epoch: 16 | Step: 20\n",
      "Loss: 0.509938836098 at Epoch: 16 | Step: 40\n",
      "Overall Average Loss: 0.534784376621 at Epoch: 16\n",
      "Loss: 0.47408130765 at Epoch: 17 | Step: 0\n",
      "Loss: 0.50886631012 at Epoch: 17 | Step: 20\n",
      "Loss: 0.510581076145 at Epoch: 17 | Step: 40\n",
      "Overall Average Loss: 0.530485510826 at Epoch: 17\n",
      "Loss: 0.470617681742 at Epoch: 18 | Step: 0\n",
      "Loss: 0.505066335201 at Epoch: 18 | Step: 20\n",
      "Loss: 0.51499325037 at Epoch: 18 | Step: 40\n",
      "Overall Average Loss: 0.52730178833 at Epoch: 18\n",
      "Loss: 0.46736240387 at Epoch: 19 | Step: 0\n",
      "Loss: 0.505021572113 at Epoch: 19 | Step: 20\n",
      "Loss: 0.512701034546 at Epoch: 19 | Step: 40\n",
      "Overall Average Loss: 0.524192869663 at Epoch: 19\n",
      "Loss: 0.470387965441 at Epoch: 20 | Step: 0\n",
      "Loss: 0.504788458347 at Epoch: 20 | Step: 20\n",
      "Loss: 0.508899629116 at Epoch: 20 | Step: 40\n",
      "Overall Average Loss: 0.523307919502 at Epoch: 20\n",
      "Loss: 0.464465528727 at Epoch: 21 | Step: 0\n",
      "Loss: 0.507555365562 at Epoch: 21 | Step: 20\n",
      "Loss: 0.515251517296 at Epoch: 21 | Step: 40\n",
      "Overall Average Loss: 0.52463054657 at Epoch: 21\n",
      "Loss: 0.469170838594 at Epoch: 22 | Step: 0\n",
      "Loss: 0.501567602158 at Epoch: 22 | Step: 20\n",
      "Loss: 0.50456982851 at Epoch: 22 | Step: 40\n",
      "Overall Average Loss: 0.526973605156 at Epoch: 22\n",
      "Loss: 0.485012233257 at Epoch: 23 | Step: 0\n",
      "Loss: 0.498085379601 at Epoch: 23 | Step: 20\n",
      "Loss: 0.495019763708 at Epoch: 23 | Step: 40\n",
      "Overall Average Loss: 0.523140490055 at Epoch: 23\n",
      "Loss: 0.466693073511 at Epoch: 24 | Step: 0\n",
      "Loss: 0.485822230577 at Epoch: 24 | Step: 20\n",
      "Loss: 0.494245409966 at Epoch: 24 | Step: 40\n",
      "Overall Average Loss: 0.514452755451 at Epoch: 24\n",
      "Loss: 0.471115678549 at Epoch: 25 | Step: 0\n",
      "Loss: 0.483766198158 at Epoch: 25 | Step: 20\n",
      "Loss: 0.489568740129 at Epoch: 25 | Step: 40\n",
      "Overall Average Loss: 0.512136101723 at Epoch: 25\n",
      "Loss: 0.474546521902 at Epoch: 26 | Step: 0\n",
      "Loss: 0.48164254427 at Epoch: 26 | Step: 20\n",
      "Loss: 0.486355632544 at Epoch: 26 | Step: 40\n",
      "Overall Average Loss: 0.511702656746 at Epoch: 26\n",
      "Loss: 0.477392196655 at Epoch: 27 | Step: 0\n",
      "Loss: 0.495191037655 at Epoch: 27 | Step: 20\n",
      "Loss: 0.496343642473 at Epoch: 27 | Step: 40\n",
      "Overall Average Loss: 0.518317103386 at Epoch: 27\n",
      "Loss: 0.456563651562 at Epoch: 28 | Step: 0\n",
      "Loss: 0.484345287085 at Epoch: 28 | Step: 20\n",
      "Loss: 0.490119546652 at Epoch: 28 | Step: 40\n",
      "Overall Average Loss: 0.513072371483 at Epoch: 28\n",
      "Loss: 0.451302081347 at Epoch: 29 | Step: 0\n",
      "Loss: 0.479845076799 at Epoch: 29 | Step: 20\n",
      "Loss: 0.487130314112 at Epoch: 29 | Step: 40\n",
      "Overall Average Loss: 0.506174564362 at Epoch: 29\n",
      "Loss: 0.451211124659 at Epoch: 30 | Step: 0\n",
      "Loss: 0.475132018328 at Epoch: 30 | Step: 20\n",
      "Loss: 0.484574496746 at Epoch: 30 | Step: 40\n",
      "Overall Average Loss: 0.502227902412 at Epoch: 30\n",
      "Loss: 0.451631844044 at Epoch: 31 | Step: 0\n",
      "Loss: 0.475101321936 at Epoch: 31 | Step: 20\n",
      "Loss: 0.487421482801 at Epoch: 31 | Step: 40\n",
      "Overall Average Loss: 0.498895674944 at Epoch: 31\n",
      "Loss: 0.447932511568 at Epoch: 32 | Step: 0\n",
      "Loss: 0.474938631058 at Epoch: 32 | Step: 20\n",
      "Loss: 0.484663814306 at Epoch: 32 | Step: 40\n",
      "Overall Average Loss: 0.497542649508 at Epoch: 32\n",
      "Loss: 0.444742619991 at Epoch: 33 | Step: 0\n",
      "Loss: 0.471693068743 at Epoch: 33 | Step: 20\n",
      "Loss: 0.484110414982 at Epoch: 33 | Step: 40\n",
      "Overall Average Loss: 0.493412494659 at Epoch: 33\n",
      "Loss: 0.442980676889 at Epoch: 34 | Step: 0\n",
      "Loss: 0.467999815941 at Epoch: 34 | Step: 20\n",
      "Loss: 0.478754997253 at Epoch: 34 | Step: 40\n",
      "Overall Average Loss: 0.492076158524 at Epoch: 34\n",
      "Loss: 0.440233498812 at Epoch: 35 | Step: 0\n",
      "Loss: 0.470312267542 at Epoch: 35 | Step: 20\n",
      "Loss: 0.476249724627 at Epoch: 35 | Step: 40\n",
      "Overall Average Loss: 0.489179372787 at Epoch: 35\n",
      "Loss: 0.439033806324 at Epoch: 36 | Step: 0\n",
      "Loss: 0.469206631184 at Epoch: 36 | Step: 20\n",
      "Loss: 0.475227326155 at Epoch: 36 | Step: 40\n",
      "Overall Average Loss: 0.49189427495 at Epoch: 36\n",
      "Loss: 0.436659693718 at Epoch: 37 | Step: 0\n",
      "Loss: 0.491647422314 at Epoch: 37 | Step: 20\n",
      "Loss: 0.472197830677 at Epoch: 37 | Step: 40\n",
      "Overall Average Loss: 0.505670905113 at Epoch: 37\n",
      "Loss: 0.457237482071 at Epoch: 38 | Step: 0\n",
      "Loss: 0.466650813818 at Epoch: 38 | Step: 20\n",
      "Loss: 0.481216132641 at Epoch: 38 | Step: 40\n",
      "Overall Average Loss: 0.500137031078 at Epoch: 38\n",
      "Loss: 0.47481316328 at Epoch: 39 | Step: 0\n",
      "Loss: 0.467495381832 at Epoch: 39 | Step: 20\n",
      "Loss: 0.470978289843 at Epoch: 39 | Step: 40\n",
      "Overall Average Loss: 0.496206909418 at Epoch: 39\n",
      "Loss: 0.461064308882 at Epoch: 40 | Step: 0\n",
      "Loss: 0.464761227369 at Epoch: 40 | Step: 20\n",
      "Loss: 0.46520626545 at Epoch: 40 | Step: 40\n",
      "Overall Average Loss: 0.491576761007 at Epoch: 40\n",
      "Loss: 0.469571202993 at Epoch: 41 | Step: 0\n",
      "Loss: 0.459188580513 at Epoch: 41 | Step: 20\n",
      "Loss: 0.462537109852 at Epoch: 41 | Step: 40\n",
      "Overall Average Loss: 0.486786901951 at Epoch: 41\n",
      "Loss: 0.469054400921 at Epoch: 42 | Step: 0\n",
      "Loss: 0.459449231625 at Epoch: 42 | Step: 20\n",
      "Loss: 0.463728100061 at Epoch: 42 | Step: 40\n",
      "Overall Average Loss: 0.484697341919 at Epoch: 42\n",
      "Loss: 0.457308411598 at Epoch: 43 | Step: 0\n",
      "Loss: 0.455283880234 at Epoch: 43 | Step: 20\n",
      "Loss: 0.461940914392 at Epoch: 43 | Step: 40\n",
      "Overall Average Loss: 0.485280305147 at Epoch: 43\n",
      "Loss: 0.457079768181 at Epoch: 44 | Step: 0\n",
      "Loss: 0.451515763998 at Epoch: 44 | Step: 20\n",
      "Loss: 0.457645446062 at Epoch: 44 | Step: 40\n",
      "Overall Average Loss: 0.482401043177 at Epoch: 44\n",
      "Loss: 0.455950021744 at Epoch: 45 | Step: 0\n",
      "Loss: 0.45035892725 at Epoch: 45 | Step: 20\n",
      "Loss: 0.46109944582 at Epoch: 45 | Step: 40\n",
      "Overall Average Loss: 0.482721477747 at Epoch: 45\n",
      "Loss: 0.450999975204 at Epoch: 46 | Step: 0\n",
      "Loss: 0.457892537117 at Epoch: 46 | Step: 20\n",
      "Loss: 0.473802834749 at Epoch: 46 | Step: 40\n",
      "Overall Average Loss: 0.484114319086 at Epoch: 46\n",
      "Loss: 0.439380735159 at Epoch: 47 | Step: 0\n",
      "Loss: 0.456071287394 at Epoch: 47 | Step: 20\n",
      "Loss: 0.470109641552 at Epoch: 47 | Step: 40\n",
      "Overall Average Loss: 0.479265272617 at Epoch: 47\n",
      "Loss: 0.43844383955 at Epoch: 48 | Step: 0\n",
      "Loss: 0.446683079004 at Epoch: 48 | Step: 20\n",
      "Loss: 0.470870375633 at Epoch: 48 | Step: 40\n",
      "Overall Average Loss: 0.478016227484 at Epoch: 48\n",
      "Loss: 0.448094546795 at Epoch: 49 | Step: 0\n",
      "Loss: 0.448546677828 at Epoch: 49 | Step: 20\n",
      "Loss: 0.494729071856 at Epoch: 49 | Step: 40\n",
      "Overall Average Loss: 0.481781721115 at Epoch: 49\n"
     ]
    }
   ],
   "source": [
    "# Set to train mode\n",
    "# model.cuda()\n",
    "model.train()\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "    N = 0\n",
    "    for step, (b_x, b_y) in enumerate(make_batch(df, batch_size=200)):\n",
    "        # print step, b_x.shape, b_y.shape\n",
    "        bsize = b_x.size(0)\n",
    "        \n",
    "        h_state = model.init_hidden(bsize, gpu=True)\n",
    "\n",
    "        pred = model(b_x, h_state)\n",
    "        loss = criterion(pred, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss\n",
    "        N += 1.0\n",
    "        if step%20 == 0:\n",
    "            print 'Loss: {} at Epoch: {} | Step: {}'.format(loss, epoch, step)\n",
    "        \n",
    "    print \"Overall Average Loss: {} at Epoch: {}\".format(total_loss / float(N), epoch)\n",
    "    \n",
    "    # Save model checkpoints\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model.state_dict(), \"/home/shubham/all_projects/CB/Summer_2018/data/checkpoints/seq_lstm_bucket/model_256h_epoch_{}.ckpt\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeqModel(\n",
       "  (rnn): LSTM(300, 256, batch_first=True)\n",
       "  (lin): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (dropout): Dropout(p=0.42)\n",
       "  (out): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make ppredictions\n",
    "model.eval()\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  1.3468e-15]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict('I am going to some place to take play a game')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 256) (1, 256)\n"
     ]
    }
   ],
   "source": [
    "model.cpu()\n",
    "\n",
    "v1 = model.get_embedding('I am going to a place')\n",
    "v2 = model.get_embedding('I am not going')\n",
    "print v1.shape, v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7183737]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.pairwise.cosine_distances(v1, v2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
